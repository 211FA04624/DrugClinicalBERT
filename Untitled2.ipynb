import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
DATA_PATH = "/content/updated_project_dataset.csv"  # Update with your path if needed
data = pd.read_csv(DATA_PATH).dropna()  # Remove empty rows

# Count occurrences of each diagnosis
disease_counts = data["Diagnosis"].value_counts()
temp_df = pd.DataFrame({
    "Diagnosis": disease_counts.index,
    "Counts": disease_counts.values
})

# Sort and select the top 20 diagnoses (optional)
top_n = 20  # Change as needed
temp_df = temp_df.head(top_n)

# Plot the distribution of diagnoses
plt.figure(figsize=(14, 6))  # Increase figure size
sns.barplot(x="Diagnosis", y="Counts", data=temp_df, palette="viridis")

# Improve x-axis labels
plt.xticks(rotation=60, ha="right")  # Rotate and align labels
plt.xlabel("Diagnosis", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.title("Top Diagnoses Distribution", fontsize=14)
plt.grid(axis="y", linestyle="--", alpha=0.7)  # Add grid for better readability

plt.show()

# Load the dataset
df = pd.read_csv('/content/updated_project_dataset.csv')

# Display the first few rows of the dataset
print(df.head())

# Check for missing values
print(df.isnull().sum())

# Encode categorical variables
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Define features and target variable
X = df.drop('Indication', axis=1)
y = df['Indication']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pip install transformers torch scikit-learn


from transformers import BertTokenizer, BertForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset

# Load ClinicalBERT tokenizer and model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y_train)))

# Convert numerical features to text format (Fixes mismatch)
X_train_text = X_train.apply(lambda row: f"Age: {row['Age']}, Gender: {row['Gender']}, Diagnosis: {row['Diagnosis']}", axis=1)
X_test_text = X_test.apply(lambda row: f"Age: {row['Age']}, Gender: {row['Gender']}, Diagnosis: {row['Diagnosis']}", axis=1)

# Tokenize the input data
X_train_tokens = tokenizer(list(X_train_text), padding=True, truncation=True, max_length=512, return_tensors="pt")
X_test_tokens = tokenizer(list(X_test_text), padding=True, truncation=True, max_length=512, return_tensors="pt")

# Convert labels to tensors (Ensure same length as input)
y_train_tensor = torch.tensor(y_train.values[:len(X_train_tokens["input_ids"])])
y_test_tensor = torch.tensor(y_test.values[:len(X_test_tokens["input_ids"])])

# Debug sizes (optional)
print(f"X_train_tokens size: {X_train_tokens['input_ids'].shape}")
print(f"y_train_tensor size: {y_train_tensor.shape}")

# Create DataLoader for batch processing
train_data = TensorDataset(X_train_tokens["input_ids"], X_train_tokens["attention_mask"], y_train_tensor)
train_loader = DataLoader(train_data, batch_size=16, shuffle=True)


import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler
from sklearn.model_selection import train_test_split

# Set device (Use GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load dataset
df = pd.read_csv("/content/updated_project_dataset.csv")

# Rename column for consistency
df.rename(columns={"Name of Drug": "Drug"}, inplace=True)

# Convert Age column to numeric
df["Age"] = pd.to_numeric(df["Age"], errors='coerce')

# Ensure dataset has required columns
required_columns = ["Age", "Gender", "Diagnosis", "Drug"]
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Dataset must contain {required_columns} columns.")

# Split into train and test (80% train, 20% test)
X = df[["Age", "Gender", "Diagnosis"]]
y = df["Drug"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Load ClinicalBERT model
tokenizer = BertTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
model = BertForSequenceClassification.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", num_labels=len(y_train.unique()))
model.to(device)

# Convert numerical features into text
X_train_text = X_train.apply(lambda row: f"Age: {row['Age']}, Gender: {row['Gender']}, Diagnosis: {row['Diagnosis']}", axis=1)
X_test_text = X_test.apply(lambda row: f"Age: {row['Age']}, Gender: {row['Gender']}, Diagnosis: {row['Diagnosis']}", axis=1)

# Tokenization
X_train_tokens = tokenizer(list(X_train_text), padding=True, truncation=True, max_length=512, return_tensors="pt")
X_test_tokens = tokenizer(list(X_test_text), padding=True, truncation=True, max_length=512, return_tensors="pt")

# Convert labels to categorical IDs
drug_to_id = {drug: i for i, drug in enumerate(y_train.unique())}
y_train_tensor = torch.tensor(y_train.map(drug_to_id).values, dtype=torch.long).to(device)
y_test_tensor = torch.tensor(y_test.map(drug_to_id).values, dtype=torch.long).to(device)

# Create DataLoader
train_data = TensorDataset(X_train_tokens["input_ids"].to(device), X_train_tokens["attention_mask"].to(device), y_train_tensor)
test_data = TensorDataset(X_test_tokens["input_ids"].to(device), X_test_tokens["attention_mask"].to(device), y_test_tensor)
train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
test_loader = DataLoader(test_data, batch_size=16, shuffle=False)

# Optimizer and Loss function
class_counts = torch.bincount(y_train_tensor)
class_weights = 1.0 / class_counts.float()
class_weights = class_weights.to(device)
loss_fn = nn.CrossEntropyLoss(weight=class_weights)
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 5)

# Training Loop
epochs = 10
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}")

# Model Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=1)
        correct += (predictions == labels).sum().item()
        total += labels.size(0)
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")


import torch

# Save the trained ClinicalBERT model and tokenizer
model.save_pretrained("clinical_bert_model")
tokenizer.save_pretrained("clinical_bert_model")

# Save the optimizer and scheduler (optional)
torch.save(optimizer.state_dict(), "clinical_bert_model/optimizer.pt")
torch.save(scheduler.state_dict(), "clinical_bert_model/scheduler.pt")


from transformers import BertForSequenceClassification, BertTokenizer

# Load the model and tokenizer
model = BertForSequenceClassification.from_pretrained("clinical_bert_model")
tokenizer = BertTokenizer.from_pretrained("clinical_bert_model")

# Move the model to the appropriate device (CPU/GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler

# Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the dataset
df = pd.read_csv('updated_project_dataset.csv')

# Drop unnecessary columns
df = df.drop(columns=["Date of Data Entry"], errors="ignore")  # Drop non-relevant column

# Check for missing values
df = df.dropna()  # Remove rows with missing values

# Encode target labels
label_encoder = LabelEncoder()
df['Indication'] = label_encoder.fit_transform(df['Indication'])  # Encode target column

# Convert categorical features into text format
X = df.drop('Indication', axis=1)
y = df['Indication']

X_text = X.apply(lambda row: f"Age: {row['Age']}, Gender: {row['Gender']}, Diagnosis: {row['Diagnosis']}, Drug: {row['Name of Drug']}", axis=1)

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)

# Load ClinicalBERT tokenizer and model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(df['Indication'].unique()))
model.to(device)

# Tokenize input text
X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, max_length=512, return_tensors="pt")
X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, max_length=512, return_tensors="pt")

# Convert labels to tensors
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)

# Create DataLoader
train_data = TensorDataset(X_train_tokens["input_ids"].to(device), X_train_tokens["attention_mask"].to(device), y_train_tensor)
test_data = TensorDataset(X_test_tokens["input_ids"].to(device), X_test_tokens["attention_mask"].to(device), y_test_tensor)

train_loader = DataLoader(train_data, batch_size=8, shuffle=True)  # Reduced batch size
test_loader = DataLoader(test_data, batch_size=8, shuffle=False)

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)  # Reduced learning rate
scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)

# Loss function
loss_fn = nn.CrossEntropyLoss()

# Freeze base model layers for faster training (Optional)
for param in model.base_model.parameters():
    param.requires_grad = False

# Training Loop (Reduced Epochs)
epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}, Loss: {avg_loss:.4f}")

# Evaluation
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=1)

        correct += (predictions == labels).sum().item()
        total += labels.size(0)

accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

# Save the ClinicalBERT model
model.save_pretrained("clinical_bert_model")
tokenizer.save_pretrained("clinical_bert_model")

# Save label encoder
import joblib
joblib.dump(label_encoder, "clinical_bert_model/label_encoder.pkl")

print("Model saved successfully!")


import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

# Load dataset
df = pd.read_csv("updated_project_dataset.csv")

# Drop irrelevant columns
df = df.drop(columns=["Date of Data Entry"], errors="ignore")

# Handle missing values
df = df.dropna()

# Encode categorical variables
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])  # Convert categorical to numeric
    label_encoders[column] = le

# Define features and labels
X = df.drop(columns=["Indication"])  # Features
y = df["Indication"]  # Target variable

# Convert y to categorical if needed
y = keras.utils.to_categorical(y, num_classes=len(np.unique(y)))  # One-hot encoding

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the model
model = Sequential([
    keras.layers.Input(shape=(X_train.shape[1],)),  # Ensure correct input shape
    Dense(128, activation='relu'),
    Dropout(0.3),  # Prevents overfitting
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(y_train.shape[1], activation='softmax')  # Output layer (softmax for multi-class classification)
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',  # Suitable for multi-class classification
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32,
                    validation_data=(X_test, y_test), verbose=1)

# Evaluate the model
accuracy = model.evaluate(X_test, y_test)[1]
print(f"Model Accuracy: {accuracy * 100:.2f}%")


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Predict class probabilities
y_pred_probs = model.predict(X_test)

# Convert one-hot encoded predictions and true labels back to class indices
y_pred_classes = np.argmax(y_pred_probs, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_true_classes, y_pred_classes)

# Identify the most misclassified class
misclassified_counts = cm.sum(axis=1) - np.diag(cm)  # Row sum minus correct predictions
most_misclassified_class = np.argmax(misclassified_counts)

# Identify the most wrongly predicted class
most_predicted_wrongly = np.argmax(cm.sum(axis=0) - np.diag(cm))

# Plot confusion matrix
plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", linewidths=0.5, square=True)

# Fix axis labels for better readability
plt.xlabel("Predicted Label", fontsize=14)
plt.ylabel("True Label", fontsize=14)
plt.title("Confusion Matrix", fontsize=16)

# Rotate x-axis and y-axis labels
plt.xticks(rotation=45, ha="right", fontsize=12)
plt.yticks(rotation=0, fontsize=12)

plt.show()

# Print the most misclassified class and the most wrongly predicted class
print(f"Most misclassified class: {most_misclassified_class} (Highest misclassification)")
print(f"Most wrongly predicted as another class: {most_predicted_wrongly} (Most wrong predictions)")


